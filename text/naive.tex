The time-consuming part of probabilistic skyline query is the sum of weights of all instances that dominate a specific instance (Equation 3 illustrates). In our assumption, the dataset is too large to fit into one machine's memory; in fact, all data is distributed in disks of all machines in one cluster. The most straightforward solution is partitioning objects in a pair-wise way, computing probabilistic skyline between every pair of two objects in Equation 2 and each instance \(p\)'s skyline probability \(SKYProb(p)\) in Equation 3, merging all instances' intermediate results to the final skyline probability in Equation 4. In this section, we apply the intuitive idea to three-phase and two-phase MapReduce solutions. The difference between the two approach is that the second and third phase in three-phase MapReduce framework could be merged to one.

\subsection{Three-phase MapReduce}
The three-phase MapReduce processing follows the basic computation process of probabilistic skyline query. Assume that the number of objects is \(m\), and the number of instances is \(n\). In the first phase, we create pair-wise comparisons between any two objects. Every pair of two distinct objects is assigned one key, and the number of pairs is \(C_{m}^{2}\). Say an object $o_i$ make up pairs. The replication for $o_i$ is performed $m-1$ times, in order to let $o_i$ forms pairs with all other objects. The map stage collects all objects and assigns a key for every pair, and transmit all pairs to the reduce phase.
In the reduce phase, it fetches an object pair (one of $C_{m}^{2}$ pairs), and two block nested loop comparisons are performed. Say the objects are \(P\) and \(Q\). For every q in Q, $Pr(P\nprec q) $ is computed (Equation~\ref{equ:2}). Reversely, we compute $Pr( Q\nprec p) $for every p in P. The results from all reducers are written into HDFS files.


In the second phase, our target is to obtain instance skyline probability. we read the output from the first phase and use the instance ID key as the partitioning key at the end of Map phase. Then every reducer groups all intermediate results for each instance and do an multiplication of them (Equation~\ref{equ:3}). The output from the second phase reducers is written into \(n\) (\(n\) is the number of instances of all objects) files in HDFS.

Similarly, in the third phase, output generated by the second phase is read by map function and every object ID is assigned as a new partitioning key for reduce phase. In the Reduce function, We sum all instances' skyline probability for one object (Equation~\ref{equ_final}) as the final result, that is, the final object skyline probability. Then $p-$skyline set is obtained easily by filtering if some object's skyline probability is larger than $p$.

\subsection{Two-phase MapReduce}
In the three-phase MapReduce framework, many similar operations in the second and third phase are conducted, and it suffers from high I/O overhead. It gives us the motivation that the two-phase MapReduce framework is created.

The first phase in Two-phase MapReduce is the same as in the Three-phase MapReduce. It generates \(C_{m}^{2}\) HDFS files, which contain the local skyline probability results of every pair of objects. In the second phase, these files are read in the map function, and the object ID is assigned as the partitioning key for the reduce phase. In the reduce phase, all intermediate skyline results (from Equation~\ref{equ:2}) are grouped for each instance. Then we compute instance skyline probability by Equation~\ref{equ:3}. After that, the object skyline probability computation is conducted by the sum of the product of every instance's \(Skyprob\) and its own existing \(Prob\). Then $p-$skyline set is obtained.

It is easily found that the efficiency in two-phase framework is much higher than in three-phase one.
\subsection{Restrictions}
There are several restrictions of this baseline method. Firstly, how to partition objects evenly is difficult, since the number of instances in one object might be highly different from ones in other object. For example, some object might occupy billions of instances, while some other object occupies dozens; Secondly, we don't know the distribution of instances of one object in advance. If all the instance of one object always appear in the right-corner of coordinate axis, and the object is certain to be a non-skyline object. The optimal approach should be that the object is pruned in the early stage; Thirdly, for every object, we must duplicate all instances into a machine, which is obviously inefficient. The I/O cost is obviously highly expensive. The detailed analysis will be in the later sections.


