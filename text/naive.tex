It can be found that the arduous part of computing probabilistic skyline is obtaining the sum of weights of all instances that dominate a specific instance in Equation 3. Notice, in our assumption, the dataset is too large to fit into one machine's memory; in fact, all data is distributed in disks of all machines in one cluster. The most straightforward solution is partitioning objects in a pair-wise way, computing probabilistic skyline between one pair of objects in Equation 2 and one instance \(p\)'s skyline probability \(SKYProb(p)\) in Equation 3, merging all instances' intermediate results to the final skyline probability in Equation 4. In this section, we apply the idea to a three-phase and a two-phase MapReduce frameworks.


\subsection{Three-phase MapReduce}
In the first phase of three-phase MapReduce, we create pair-wise comparisons between any two objects. At the end of Map phase, every possible pair of two objects is assigned one key. Assume that the number of objects is \(m\), and the number of the whole instances is \(n\). Therefore, the number of pairs is going to be \(C_{m}^{2}\). In the reduce phase, it fetches the object pair with the key (one of \(C_{m}^{2}\)), say the objects are \(P\) and \(Q\), and performs two block nested loop comparisons. We compute the sum of all instances' weights of P which dominate one instance of Q (Equation2). That is, for every q in Q, $Pr( P\nprec q) $ is computed. Reversely, we compute the value of $Pr( Q\nprec p) $for every p in P.  The results from all reducers are written into \(C_{m}^{2}\) HDFS files.

%In the first phase, all instances of one object are replicated in \(m-1\) bucket for a reduce function, and local probability computing is executed between two objects for every instance in \(P\) and \(Q\).

In the second phase, our target is to find the global skyline probability for each instance. So we read the output from the first phase and use the instance ID key as the partitioning key at the end of Map phase. Hence, every reducer will group all object skyline probability for each instance and do an multiplication of them (Equation3). The output from second phase reducers are written into \(n\)(\(n\) is the whole number of instances of all objects) files in HDFS.

Similarly, in the third phase, output generated by the second phase is read by map function and every object ID is assigned as a new partitioning key for reduce phase. In the Reduce function, We sum all instance's global skyline probability for one object (Equation4) as the final result, that is, the final object skyline probability.

\subsection{Two-phase MapReduce}
For the three-phase MapReduce algorithms, in most times, we have many similar operations in the second and third phase, and it suffer from high I/O overhead. It give us the incentive that we create the two-phase MapReduce function which merge the the process of Equation3 and Equation4.

The first phase in Two-phase MapReduce is the same as in the Three-phase MapReduce. It generates \(C_{m}^{2}\) HDFS files, which contain the local skyline results of every pair of objects. In the second phase, These files are read in the map function, and the object ID is assigned as the partitioning key for the reduce phase; then, in the reduce phase, we create a local hash for every instance read: instance ID is the key in hashing, and instance probability array is its value. Then we compute every instance's global skyline probability by multiplication of all values of one specific instance. After that, the object skyline probability is obtained by the sum of the product of every instance's \(Skyprob\) and its own existing \(Prob\).

It is easily found that efficiency in two-phase algorithms is much higher than in three-phase algorithms.
\subsection{Challenge}
However, it has several restrictions for this approach: (1). how to partition objects evenly is hard, since the number of instances might be highly different in various object. In addition, some object might occupy billions of instances, while some other object occupies dozens; (2). besides, we don't know the distribution of instances of one object in advance. If all the instance of one object always appear in the right-corner of coordinate axis, and the object is certain to be a non-skyline object. The optimal approach should be that the object is pruned in the early stage; (3). for every object, we must put every instance into the machine, which is obviously inefficient. the communication cost of putting every instance into one machine is too expensive, especially the IO cost is high. the detailed analysis is in the next paragraph. One improvement is indexing all points before query begins.



\subsection{Experimental Evaluation}
To compare the performance between three-phase, two-phase Hadoop implementations in one cluster and the straightforward implementation on a single machine, we implemented the three algorithms and conducted a experimental evaluation.

There're several observations found in the experiment:


1). Let me first introduce the result of single-machine algorithm. With increasing size of the dataset, the computing time is increasing exponentially. And the most arduous part in the whole computing procedure is in Equation2.

In the first experiment set, we randomly generated 100 objects, and 1 thousand instances per object. The whole computing procedure for Equation2 involves computing the local skyline probability between any pair of two objects. That is 215.6 seconds in our result. In addition, it needs 0.2s for Equation3 process and 0.02s for Equation4 process. Therefore, the whole running time is less than 216s. It can be found that 99\% of the time lies on Equation2. However, in our hadoop implementation, the whole query time is about 15 mins. Therefore, if the dataset is small, the time consumed in Hadoop is less than one in single-machine algorithm.

In the second experiment set, we randomly generated 100 objects, and 10 thousand instances per object. The test shows that it needs 5 seconds to run Equation2 for every pair of objects. Since we have \(C_{100}^{2}\) pairs, and the time in all is estimated to be more than 7 hours. However, the time consumed in the hadoop implementation is about 4 hours.

2). When compared between two Hadoop implementations, the two-phase Mapreduce framework is always faster than three-phase one. The reason is that the amount of computing is little in the third phase of the three-phase framework, and merging the second and third phase into one phase reduces one copy of reading data from the HDFS to memory, increasing I/O efficiency.
