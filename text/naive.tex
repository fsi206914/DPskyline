The time-comsuming part of probabilistic skyline query is the sum of weights of all instances that dominate a specific instance (Equation 3 illustrates). In our assumption, the dataset is too large to fit into one machine's memory; in fact, all data is distributed in disks of all machines in one cluster. The most straightforward solution is partitioning objects in a pair-wise way, computing probabilistic skyline between every pair of two objects in Equation 2 and each instance \(p\)'s skyline probability \(SKYProb(p)\) in Equation 3, merging all instances' intermediate results to the final skyline probability in Equation 4. In this section, we apply the intuitive idea to three-phase and two-phase MapReduce solutions. The difference between the two approach is that one phase in three-phase MapReduce solution could be merged to subsequent phase.

\subsection{Three-phase MapReduce}
In the first phase of three-phase MapReduce, we create pair-wise comparisons between any two objects. Assume that the number of objects is \(m\), and the number of the whole instances is \(n\). Every pair of two distinct objects is assigned one key, and the number of pairs is going to be \(C_{m}^{2}\). Say an object $o_i$ make up pairs. The replication for $o_i$ is performed $m-1$ times, in order to let $o_i$ forms pairs with other objects. The map stage collects all objects and assigns multiple keys for every object to form all pairs, and transmit all pairs to the reduce phase.
In the reduce phase, it fetches an object pair (one of $C_{m}^{2}$ pairs), and two block nested loop comparisons are performed. Say the objects are \(P\) and \(Q\). We compute the sum of all instances' weights of P which dominates one instance of Q (Equation2). That is, for every q in Q, $Pr( P\nprec q) $ is computed. Reversely, we compute the value of $Pr( Q\nprec p) $for every p in P. The results from all reducers are written into \(C_{m}^{2}\) HDFS files.


In the second phase, our target is to find the global skyline probability for each instance. So we read the output from the first phase and use the instance ID key as the partitioning key at the end of Map phase. Then every reducer will group all object skyline probability for each instance and do an multiplication of them (Equation3). The output from second phase reducers is written into \(n\)(\(n\) is the whole number of instances of all objects) files in HDFS.

Similarly, in the third phase, output generated by the second phase is read by map function and every object ID is assigned as a new partitioning key for reduce phase. In the Reduce function, We sum all instance's global skyline probability for one object (Equation4) as the final result, that is, the final object skyline probability.

\subsection{Two-phase MapReduce}
For the three-phase MapReduce algorithms, in most times, we have many similar operations in the second and third phase, and it suffer from high I/O overhead. It give us the incentive that we create the two-phase MapReduce function which merge the the process of Equation3 and Equation4.

The first phase in Two-phase MapReduce is the same as in the Three-phase MapReduce. It generates \(C_{m}^{2}\) HDFS files, which contain the local skyline results of every pair of objects. In the second phase, These files are read in the map function, and the object ID is assigned as the partitioning key for the reduce phase; then, in the reduce phase, we create a local hash for every instance read: instance ID is the key in hashing, and instance probability array is its value. Then we compute every instance's global skyline probability by multiplication of all values of one specific instance. After that, the object skyline probability is obtained by the sum of the product of every instance's \(Skyprob\) and its own existing \(Prob\).

It is easily found that efficiency in two-phase algorithms is much higher than in three-phase algorithms.
\subsection{Restrictions}
There are several restrictions of this baseline method. Firstly, how to partition objects evenly is difficult, since the number of instances in one object might be highly different from ones in other object. In addition, some object might occupy billions of instances, while some other object occupies dozens; Secondly, we don't know the distribution of instances of one object in advance. If all the instance of one object always appear in the right-corner of coordinate axis, and the object is certain to be a non-skyline object. The optimal approach should be that the object is pruned in the early stage; Thirdly, for every object, we must duplicate all instances into a machine, which is obviously inefficient. The I/O cost is obviously highly expensive. The detailed analysis is in the next paragraph. One improvement is indexing all points before query begins.


